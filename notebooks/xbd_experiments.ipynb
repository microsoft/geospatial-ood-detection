{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Microsoft Corporation.\n",
    "\n",
    "Licensed under the MIT License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with xBD dataset under 5 settings:\n",
    "- Same Type Far: nepal_flood_post_midwest_flood_post\n",
    "- Same Type Close: santa-rosa-wildfire-post_woolsey-fire-pos\n",
    "- Different Type Far: hurricane-matthew_post_nepal_flood_post\n",
    "- Different Type Close: hurricane-matthew_mexico-earthquake\n",
    "- Pre Post: portugal-wildfire-pre-post\n",
    "\n",
    "For each, we perform: \n",
    "- layer benchmarking\n",
    "- activation extraction\n",
    "- g training and evaluation\n",
    "- g_hat training and evaluation\n",
    "- g and g_hat statistical significance test\n",
    "- g benchmark\n",
    "- clustering benchmark\n",
    "- num_cluster vs. g and g_hat performance investigation\n",
    "- activation space visualization    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.tardis.eurosat_xbd_utils import *\n",
    "from src.tardis.utils import *\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"Current GPU:\", torch.cuda.current_device())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "fixed_seed = 31\n",
    "set_seed(fixed_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory path\n",
    "base_dir = \"/ws/geospatial-ood-detection/configs/xview/\"\n",
    "\n",
    "# Configuration paths\n",
    "same_far_config_path = f\"{base_dir}xview_config_samedisaster_distant.yaml\"\n",
    "same_close_config_path = f\"{base_dir}xview_config_samedisaster_close.yaml\"\n",
    "different_far_config_path = f\"{base_dir}xview_config_differentdisaster_distant.yaml\"\n",
    "different_close_config_path = f\"{base_dir}xview_config_differentdisaster_close.yaml\"\n",
    "pre_post_config_path = f\"{base_dir}xview_config_prepost.yaml\"\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Parameters\n",
    "collect_activations_from_layers = [\"encoder.layer2.0.conv1\"]\n",
    "getitem_keys = [\"image\", \"mask\"]\n",
    "n_batches_to_process = 10\n",
    "downsample_method = \"avg_pool\"\n",
    "verbose = False\n",
    "\n",
    "# Downsample methods\n",
    "downsample_methods = [\"avg_pool\", \"mean_std\", \"avg_pool\", \"max_pool\", \"nodownsample\"]\n",
    "downsample_benchmark = {}\n",
    "\n",
    "# Benchmark settings\n",
    "test_size = 0.2\n",
    "n_estimators = 100\n",
    "split_seed = 31\n",
    "fixed_classifier_seed = 31\n",
    "layer_benchmark = {}\n",
    "\n",
    "# Selected layers\n",
    "model, _ = get_model_config(same_far_config_path, base_dir, device)\n",
    "all_layer_names = get_all_layer_names(model)\n",
    "selected_layers = pick_random_layers(all_layer_names, 10)\n",
    "selected_layers = [[layer] for layer in selected_layers]\n",
    "print(\"Selected 10 layers:\", selected_layers)\n",
    "\n",
    "# Optuna settings\n",
    "n_optuna_trials = 20\n",
    "min_cluster = 2\n",
    "max_cluster_ratio = 0.3\n",
    "min_fraction = 0.01\n",
    "max_fraction = 0.2\n",
    "fixed_seed = 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same type - Far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, cfg = get_model_config(same_far_config_path, device)\n",
    "datamodule, datamodule_train, datamodule_val, datamodule_test = prepare_datamodule(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nametag = (\n",
    "    cfg.id_ood_disaster[0][\"disaster_name\"]\n",
    "    + \"-\"\n",
    "    + cfg.id_ood_disaster[0][\"pre-post\"]\n",
    "    + \"_\"\n",
    "    + cfg.id_ood_disaster[1][\"disaster_name\"]\n",
    "    + \"-\"\n",
    "    + cfg.id_ood_disaster[1][\"pre-post\"]\n",
    ")\n",
    "\n",
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=datamodule_train,\n",
    "        test_dataloader=datamodule_test,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = nametag + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, nametag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=datamodule_train,\n",
    "        test_dataloader=datamodule_test,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = nametag + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, nametag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "    model=model,\n",
    "    dm=datamodule,\n",
    "    train_dataloader=datamodule_train,\n",
    "    test_dataloader=datamodule_test,\n",
    "    layer_names=collect_activations_from_layers,\n",
    "    device=device,\n",
    "    getitem_keys=getitem_keys,\n",
    "    n_batches_to_process=n_batches_to_process,\n",
    "    downsample_method=downsample_method,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nametag = (\n",
    "    cfg.id_ood_disaster[0][\"disaster_name\"]\n",
    "    + \"-\"\n",
    "    + cfg.id_ood_disaster[0][\"pre-post\"]\n",
    "    + \"_\"\n",
    "    + cfg.id_ood_disaster[1][\"disaster_name\"]\n",
    "    + \"-\"\n",
    "    + cfg.id_ood_disaster[1][\"pre-post\"]\n",
    ")\n",
    "\n",
    "print(\"confidence_intervals_g\", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k_condidence_g_hat(\n",
    "    X,\n",
    "    y,\n",
    "    M,\n",
    "    test_size=0.2,\n",
    "    n_runs=10,\n",
    "    confidence_level=0.95,\n",
    "    confidence_intervals_g=confidence_intervals_g,\n",
    "    clf=None,\n",
    "    save_plot=True,\n",
    "    fname=nametag,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster, y_train_cluster, y_clusters, class_name=nametag, save_plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same type - Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, cfg = get_model_config(same_close_config_path, device)\n",
    "datamodule, datamodule_train, datamodule_val, datamodule_test = prepare_datamodule(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nametag = (\n",
    "    cfg.id_ood_disaster[0][\"disaster_name\"]\n",
    "    + \"-\"\n",
    "    + cfg.id_ood_disaster[0][\"pre-post\"]\n",
    "    + \"_\"\n",
    "    + cfg.id_ood_disaster[1][\"disaster_name\"]\n",
    "    + \"-\"\n",
    "    + cfg.id_ood_disaster[1][\"pre-post\"]\n",
    ")\n",
    "\n",
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=datamodule_train,\n",
    "        test_dataloader=datamodule_test,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = nametag + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, nametag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=datamodule_train,\n",
    "        test_dataloader=datamodule_test,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = nametag + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, nametag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layer_names = get_all_layer_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_activations_from_layers = [\"encoder.layer2.0.conv1\"]\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "getitem_keys = [\"image\", \"mask\"]\n",
    "n_batches_to_process = n_batches_to_process\n",
    "downsample_method = \"avg_pool\"\n",
    "verbose = False\n",
    "\n",
    "X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "    model=model,\n",
    "    dm=datamodule,\n",
    "    train_dataloader=datamodule_train,\n",
    "    test_dataloader=datamodule_test,\n",
    "    layer_names=collect_activations_from_layers,\n",
    "    device=device,\n",
    "    getitem_keys=getitem_keys,\n",
    "    n_batches_to_process=n_batches_to_process,\n",
    "    downsample_method=downsample_method,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g\", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k_condidence_g_hat(\n",
    "    X,\n",
    "    y,\n",
    "    M,\n",
    "    test_size=0.2,\n",
    "    n_runs=10,\n",
    "    confidence_level=0.95,\n",
    "    confidence_intervals_g=confidence_intervals_g,\n",
    "    clf=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster, y_train_cluster, y_clusters, class_name=\"Disaster\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different type - Far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, cfg = get_model_config(different_far_config_path, device)\n",
    "datamodule, datamodule_train, datamodule_val, datamodule_test = prepare_datamodule(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nametag = (\n",
    "    cfg.id_ood_disaster[0][\"disaster_name\"]\n",
    "    + \"-\"\n",
    "    + cfg.id_ood_disaster[0][\"pre-post\"]\n",
    "    + \"_\"\n",
    "    + cfg.id_ood_disaster[1][\"disaster_name\"]\n",
    "    + \"-\"\n",
    "    + cfg.id_ood_disaster[1][\"pre-post\"]\n",
    ")\n",
    "\n",
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=datamodule_train,\n",
    "        test_dataloader=datamodule_test,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = nametag + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, nametag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=datamodule_train,\n",
    "        test_dataloader=datamodule_test,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = nametag + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, nametag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layer_names = get_all_layer_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_activations_from_layers = [\"encoder.layer2.0.conv1\"]\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "getitem_keys = [\"image\", \"mask\"]\n",
    "n_batches_to_process = n_batches_to_process\n",
    "downsample_method = \"avg_pool\"\n",
    "verbose = False\n",
    "\n",
    "X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "    model=model,\n",
    "    dm=datamodule,\n",
    "    train_dataloader=datamodule_train,\n",
    "    test_dataloader=datamodule_test,\n",
    "    layer_names=collect_activations_from_layers,\n",
    "    device=device,\n",
    "    getitem_keys=getitem_keys,\n",
    "    n_batches_to_process=n_batches_to_process,\n",
    "    downsample_method=downsample_method,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nametag = (\n",
    "    cfg.id_ood_disaster[0][\"disaster_name\"]\n",
    "    + \"-\"\n",
    "    + cfg.id_ood_disaster[0][\"pre-post\"]\n",
    "    + \"_\"\n",
    "    + cfg.id_ood_disaster[1][\"disaster_name\"]\n",
    "    + \"-\"\n",
    "    + cfg.id_ood_disaster[1][\"pre-post\"]\n",
    ")\n",
    "\n",
    "print(\"confidence_intervals_g\", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k_condidence_g_hat(\n",
    "    X,\n",
    "    y,\n",
    "    M,\n",
    "    test_size=0.2,\n",
    "    n_runs=10,\n",
    "    confidence_level=0.95,\n",
    "    confidence_intervals_g=confidence_intervals_g,\n",
    "    clf=None,\n",
    "    fname=nametag,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster, y_train_cluster, y_clusters, class_name=nametag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different type - Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, cfg = get_model_config(different_close_config_path, device)\n",
    "datamodule, datamodule_train, datamodule_val, datamodule_test = prepare_datamodule(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nametag = (\n",
    "    cfg.id_ood_disaster[0][\"disaster_name\"]\n",
    "    + \"-\"\n",
    "    + cfg.id_ood_disaster[0][\"pre-post\"]\n",
    "    + \"_\"\n",
    "    + cfg.id_ood_disaster[1][\"disaster_name\"]\n",
    "    + \"-\"\n",
    "    + cfg.id_ood_disaster[1][\"pre-post\"]\n",
    ")\n",
    "\n",
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=datamodule_train,\n",
    "        test_dataloader=datamodule_test,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = nametag + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, nametag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=datamodule_train,\n",
    "        test_dataloader=datamodule_test,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = nametag + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, nametag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layer_names = get_all_layer_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_activations_from_layers = [\"encoder.layer2.0.conv1\"]\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "getitem_keys = [\"image\", \"mask\"]\n",
    "n_batches_to_process = n_batches_to_process\n",
    "downsample_method = \"avg_pool\"\n",
    "verbose = False\n",
    "\n",
    "X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "    model=model,\n",
    "    dm=datamodule,\n",
    "    train_dataloader=datamodule_train,\n",
    "    test_dataloader=datamodule_test,\n",
    "    layer_names=collect_activations_from_layers,\n",
    "    device=device,\n",
    "    getitem_keys=getitem_keys,\n",
    "    n_batches_to_process=n_batches_to_process,\n",
    "    downsample_method=downsample_method,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g\", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k_condidence_g_hat(\n",
    "    X,\n",
    "    y,\n",
    "    M,\n",
    "    test_size=0.2,\n",
    "    n_runs=10,\n",
    "    confidence_level=0.95,\n",
    "    confidence_intervals_g=confidence_intervals_g,\n",
    "    clf=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster, y_train_cluster, y_clusters, class_name=\"Disaster\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, cfg = get_model_config(pre_post_config_path, device)\n",
    "datamodule, datamodule_train, datamodule_val, datamodule_test = prepare_datamodule(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nametag = (\n",
    "    cfg.id_ood_disaster[0][\"disaster_name\"]\n",
    "    + \"-\"\n",
    "    + cfg.id_ood_disaster[0][\"pre-post\"]\n",
    "    + \"_\"\n",
    "    + cfg.id_ood_disaster[1][\"disaster_name\"]\n",
    "    + \"-\"\n",
    "    + cfg.id_ood_disaster[1][\"pre-post\"]\n",
    ")\n",
    "\n",
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=datamodule_train,\n",
    "        test_dataloader=datamodule_test,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = nametag + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, nametag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=datamodule_train,\n",
    "        test_dataloader=datamodule_test,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = nametag + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, nametag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layer_names = get_all_layer_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_activations_from_layers = [\"encoder.layer2.0.conv1\"]\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "getitem_keys = [\"image\", \"mask\"]\n",
    "n_batches_to_process = n_batches_to_process\n",
    "downsample_method = \"avg_pool\"\n",
    "verbose = False\n",
    "\n",
    "X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "    model=model,\n",
    "    dm=datamodule,\n",
    "    train_dataloader=datamodule_train,\n",
    "    test_dataloader=datamodule_test,\n",
    "    layer_names=collect_activations_from_layers,\n",
    "    device=device,\n",
    "    getitem_keys=getitem_keys,\n",
    "    n_batches_to_process=n_batches_to_process,\n",
    "    downsample_method=downsample_method,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g\", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k_condidence_g_hat(\n",
    "    X,\n",
    "    y,\n",
    "    M,\n",
    "    test_size=0.2,\n",
    "    n_runs=10,\n",
    "    confidence_level=0.95,\n",
    "    confidence_intervals_g=confidence_intervals_g,\n",
    "    clf=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster, y_train_cluster, y_clusters, class_name=\"Disaster\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
