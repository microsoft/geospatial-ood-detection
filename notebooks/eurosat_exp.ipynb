{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Microsoft Corporation.\n",
    "\n",
    "Licensed under the MIT License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with EuroSAT dataset under 11 settings:\n",
    "- Semantic Shift: Leave-one-class-out. Train on 9 class and test on 10 class \n",
    "- Covariate Shift: Longitude-wise split. Train on West and test on East \n",
    "\n",
    "For each, we perform: \n",
    "- activation extraction\n",
    "- downsample benchmarking\n",
    "- layer benchmarking\n",
    "- g training and evaluation\n",
    "- g_hat training and evaluation\n",
    "- g and g_hat statistical significance test\n",
    "- g benchmark\n",
    "- clustering benchmark\n",
    "- num_cluster vs. g and g_hat performance investigation\n",
    "- activation space visualization    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.tardis.eurosat_xview_utils import *\n",
    "from src.tardis.utils import set_seed\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"Current GPU:\", torch.cuda.current_device())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "fixed_seed = 31\n",
    "set_seed(fixed_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main directories for checkpoints and configs\n",
    "ckpt_main_dir = \"./exp_data/main_tardis/eurosat_exp_logs\"\n",
    "config_main_dir = \"./geospatial-ood-detection/configs\"\n",
    "\n",
    "\n",
    "# Define a function to construct the paths dynamically\n",
    "def construct_paths(main_dir, sub_dir, filename):\n",
    "    return os.path.join(main_dir, sub_dir, filename)\n",
    "\n",
    "\n",
    "# Define subdirectories and filenames for each configuration and checkpoint\n",
    "paths = {\n",
    "    \"forest\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_forest.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir, \"Holdout_Forest_resnet50_0066\", \"epoch=21-step=4972.ckpt\"\n",
    "        ),\n",
    "    },\n",
    "    \"herb_veg\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_herbaceousvegetation.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir,\n",
    "            \"Holdout_HerbaceousVegetation_resnet50_0066\",\n",
    "            \"epoch=27-step=6328.ckpt\",\n",
    "        ),\n",
    "    },\n",
    "    \"highway\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_highway.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir, \"Holdout_Highway_resnet50_0066\", \"epoch=28-step=6670.ckpt\"\n",
    "        ),\n",
    "    },\n",
    "    \"industrial\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_industrial.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir, \"Holdout_Industrial_resnet50_0066\", \"epoch=32-step=7590.ckpt\"\n",
    "        ),\n",
    "    },\n",
    "    \"pasture\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_pasture.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir, \"Holdout_Pasture_resnet50_0066\", \"epoch=34-step=8225.ckpt\"\n",
    "        ),\n",
    "    },\n",
    "    \"permanentcrop\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_permanentcrop.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir,\n",
    "            \"Holdout_PermanentCrop_resnet50_0066\",\n",
    "            \"epoch=26-step=6210.ckpt\",\n",
    "        ),\n",
    "    },\n",
    "    \"residential\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_residential.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir,\n",
    "            \"Holdout_Residential_resnet50_0066\",\n",
    "            \"epoch=49-step=11250.ckpt\",\n",
    "        ),\n",
    "    },\n",
    "    \"river\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_river.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir, \"Holdout_River_resnet50_0066\", \"epoch=31-step=7392.ckpt\"\n",
    "        ),\n",
    "    },\n",
    "    \"sealake\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_sealake.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir, \"Holdout_SeaLake_resnet50_0066\", \"epoch=34-step=7875.ckpt\"\n",
    "        ),\n",
    "    },\n",
    "    \"annualcrop\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_annualcrop.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir, \"Holdout_AnnualCrop_resnet50_0066\", \"epoch=15-step=3616.ckpt\"\n",
    "        ),\n",
    "    },\n",
    "    \"spatial_split\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_spatial_config.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir, \"eurosat_spatial_0776\", \"epoch=18-step=4826.ckpt\"\n",
    "        ),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define other parameters\n",
    "layer = [\"conv1\"]\n",
    "downsample_method = \"avg_pool\"\n",
    "getitem_keys = [\"image\", \"label\"]\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_batches_to_process = 2\n",
    "\n",
    "# Downsampling methods and benchmarks\n",
    "downsample_methods = [\"avg_pool\", \"mean_std\", \"avg_pool\", \"max_pool\", \"nodownsample\"]\n",
    "downsample_benchmark = {}\n",
    "layer_benchmark = {}\n",
    "\n",
    "collect_activations_from_layers = [\"conv1\"]\n",
    "getitem_keys = [\"image\", \"label\"]\n",
    "verbose = False\n",
    "test_size = 0.2\n",
    "n_estimators = 100\n",
    "split_seed = 31\n",
    "fixed_classifier_seed = 31\n",
    "\n",
    "n_optuna_trials = 20\n",
    "min_cluster = 2\n",
    "max_cluster_ratio = 0.2\n",
    "min_fraction = 0.01\n",
    "max_fraction = 0.2\n",
    "fixed_seed = 31\n",
    "\n",
    "# Print paths to verify\n",
    "for key, path in paths.items():\n",
    "    print(f\"{key} config path: {path['config']}\")\n",
    "    print(f\"{key} ckpt path: {path['ckpt']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_aug_input = True\n",
    "\n",
    "(\n",
    "    X,\n",
    "    y,\n",
    "    model,\n",
    "    datamodule,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    test_dataloader,\n",
    "    cfg_dict,\n",
    "    x_sample_train_tensor,\n",
    "    x_sample_test_tensor,\n",
    ") = get_X_y_arrays(\n",
    "    paths[\"forest\"][\"config\"],\n",
    "    paths[\"forest\"][\"ckpt\"],\n",
    "    layer,\n",
    "    downsample_method,\n",
    "    getitem_keys,\n",
    "    device,\n",
    "    n_batches_to_process,\n",
    "    mode=\"holdout\",\n",
    "    verbose=False,\n",
    "    collect_aug_input=collect_aug_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample_train_tensor = torch.Tensor(x_sample_train_tensor).to(\"cuda\")\n",
    "x_sample_test_tensor = torch.Tensor(x_sample_test_tensor).to(\"cuda\")\n",
    "print(x_sample_train_tensor.shape, x_sample_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Benchmark -- WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = int(0.3 * len(X))\n",
    "M = 0.1\n",
    "fixed_seed = 31\n",
    "i = 0\n",
    "test_size = 0.2\n",
    "\n",
    "metrics, X_samples, y_ground_truth, y_surrogate, train_indices, baseline_indices = (\n",
    "    run_g_hat_experiment(X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1)\n",
    ")\n",
    "\n",
    "y_random = np.random.randint(2, size=len(y_ground_truth))\n",
    "y_ground_truth.shape, y_surrogate.shape, y_random.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices.shape, baseline_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_samples = np.unique(train_indices)\n",
    "unique_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_0 = np.where(y_ground_truth == 0)[0]\n",
    "idx_1 = np.where(y_ground_truth == 1)[0]\n",
    "\n",
    "# Get the corresponding X values for y equals 0 and 1\n",
    "X_train = X_samples[idx_0]\n",
    "X_test = X_samples[idx_1]\n",
    "\n",
    "X_train_tensor = torch.Tensor(X_train)\n",
    "X_test_tensor = torch.Tensor(X_test)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_surrogate_train = y_surrogate[idx_0]\n",
    "y_surrogate_test = y_surrogate[idx_1]\n",
    "print(\"y_surrogate_train shape:\", y_surrogate_train.shape)\n",
    "print(\"y_surrogate_test shape:\", y_surrogate_test.shape)\n",
    "np.unique(y_surrogate_train, return_counts=True), np.unique(\n",
    "    y_surrogate_test, return_counts=True\n",
    ")\n",
    "\n",
    "y_surrogate_combined_labels = np.concatenate([y_surrogate_train, y_surrogate_test])\n",
    "print(\"y_surrogate_combined_labels shape:\", y_surrogate_combined_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def react(logits, threshold):\n",
    "    \"\"\"\n",
    "    Applies the ReAct method to the given logits.\n",
    "\n",
    "    Parameters:\n",
    "    - logits: Tensor of raw logits.\n",
    "    - threshold: Clipping threshold.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor of logits after applying ReAct.\n",
    "    \"\"\"\n",
    "    return torch.clamp(logits, max=threshold)\n",
    "\n",
    "\n",
    "# Evaluation Functions\n",
    "def evaluate_react(id_logits, ood_logits, replace_labels, threshold):\n",
    "    # Apply ReAct to the ID and OOD logits\n",
    "    reacted_id_logits = react(id_logits, threshold)\n",
    "    reacted_ood_logits = react(ood_logits, threshold)\n",
    "\n",
    "    # Compute scores using the maximum logit value\n",
    "    id_scores = torch.max(reacted_id_logits, dim=1)[0].cpu().numpy()\n",
    "    ood_scores = torch.max(reacted_ood_logits, dim=1)[0].cpu().numpy()\n",
    "\n",
    "    combined_scores = np.concatenate([id_scores, ood_scores])\n",
    "\n",
    "    if replace_labels is None:\n",
    "        # Create labels (1 for ID, 0 for OOD)\n",
    "        id_labels = np.ones_like(id_scores)\n",
    "        ood_labels = np.zeros_like(ood_scores)\n",
    "\n",
    "        # Combine scores and labels\n",
    "        combined_labels = np.concatenate([id_labels, ood_labels])\n",
    "    else:\n",
    "        combined_labels = replace_labels\n",
    "\n",
    "    # Calculate AUROC\n",
    "    auroc = roc_auc_score(combined_labels, combined_scores)\n",
    "\n",
    "    # Calculate FPR at 95% TPR\n",
    "    fpr, tpr, thresholds = roc_curve(combined_labels, combined_scores)\n",
    "    fpr95 = fpr[np.where(tpr >= 0.95)[0][0]]\n",
    "\n",
    "    # Calculate Accuracy (assuming a threshold of 0.5 for binary classification)\n",
    "    predicted_labels = (combined_scores >= 0.5).astype(int)\n",
    "    accuracy = accuracy_score(combined_labels, predicted_labels)\n",
    "\n",
    "    return auroc, fpr95, accuracy\n",
    "\n",
    "\n",
    "def find_optimal_threshold_react(\n",
    "    id_logits, ood_logits, threshold_range, replace_labels, metric=\"auroc\"\n",
    "):\n",
    "    best_threshold = None\n",
    "    best_metric_value = (\n",
    "        -float(\"inf\") if metric in [\"auroc\", \"accuracy\"] else float(\"inf\")\n",
    "    )\n",
    "\n",
    "    for threshold in threshold_range:\n",
    "        auroc, fpr95, accuracy = evaluate_react(\n",
    "            id_logits, ood_logits, replace_labels, threshold\n",
    "        )\n",
    "        print(\n",
    "            f\"Threshold: {threshold}, AUROC: {auroc}, FPR95: {fpr95}, Accuracy: {accuracy}\"\n",
    "        )\n",
    "\n",
    "        if metric == \"auroc\":\n",
    "            current_metric_value = auroc\n",
    "        elif metric == \"fpr95\":\n",
    "            current_metric_value = fpr95\n",
    "        elif metric == \"accuracy\":\n",
    "            current_metric_value = accuracy\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid metric specified. Choose from 'auroc', 'fpr95', or 'accuracy'.\"\n",
    "            )\n",
    "\n",
    "        if (\n",
    "            metric in [\"auroc\", \"accuracy\"] and current_metric_value > best_metric_value\n",
    "        ) or (metric == \"fpr95\" and current_metric_value < best_metric_value):\n",
    "            best_metric_value = current_metric_value\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print(\n",
    "        f\"Optimal Threshold: {best_threshold} with {metric.upper()}: {best_metric_value}\"\n",
    "    )\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observe_metric_search = \"accuracy\"\n",
    "results = {}\n",
    "# Define a range of thresholds to test for ReAct\n",
    "threshold_range = np.linspace(0.1, 1500.0, num=50)\n",
    "\n",
    "labels = {\"gt\": None, \"surr\": y_surrogate_combined_labels, \"rand\": y_random}\n",
    "\n",
    "for key, value in labels.items():\n",
    "    replace_labels = value\n",
    "    print(f\"Replace Labels: {key}\")\n",
    "    # Find the optimal threshold for ReAct\n",
    "    optimal_threshold_react = find_optimal_threshold_react(\n",
    "        X_train_tensor,\n",
    "        X_test_tensor,\n",
    "        threshold_range,\n",
    "        replace_labels=replace_labels,\n",
    "        metric=observe_metric_search,\n",
    "    )\n",
    "    # Evaluate ReAct with the optimal threshold\n",
    "    auroc_react, fpr95_react, accuracy_react = evaluate_react(\n",
    "        X_train_tensor,\n",
    "        X_test_tensor,\n",
    "        replace_labels=replace_labels,\n",
    "        threshold=optimal_threshold_react,\n",
    "    )\n",
    "    print(\n",
    "        f\"ReAct - AUROC: {auroc_react}, FPR95: {fpr95_react}, Accuracy: {accuracy_react}\"\n",
    "    )\n",
    "\n",
    "    results[key] = {\"Accuracy\": accuracy_react}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply ASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASH Functions\n",
    "\n",
    "\n",
    "def ash_p(logits, alpha):\n",
    "    \"\"\"\n",
    "    Applies the ASH-P (Positive) method to the given logits.\n",
    "\n",
    "    Parameters:\n",
    "    - logits: Tensor of raw logits.\n",
    "    - alpha: Scaling parameter.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor of logits after applying ASH-P.\n",
    "    \"\"\"\n",
    "    return logits * (logits >= alpha).float()\n",
    "\n",
    "\n",
    "def ash_s(logits, alpha):\n",
    "    \"\"\"\n",
    "    Applies the ASH-S (Symmetric) method to the given logits.\n",
    "\n",
    "    Parameters:\n",
    "    - logits: Tensor of raw logits.\n",
    "    - alpha: Scaling parameter.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor of logits after applying ASH-S.\n",
    "    \"\"\"\n",
    "    return logits * (logits.abs() >= alpha).float()\n",
    "\n",
    "\n",
    "def ash_b(logits, alpha):\n",
    "    \"\"\"\n",
    "    Applies the ASH-B (Batch) method to the given logits.\n",
    "\n",
    "    Parameters:\n",
    "    - logits: Tensor of raw logits.\n",
    "    - alpha: Scaling parameter.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor of logits after applying ASH-B.\n",
    "    \"\"\"\n",
    "    batch_mean = logits.mean(dim=0, keepdim=True)\n",
    "    batch_std = logits.std(dim=0, keepdim=True)\n",
    "    return logits * ((logits - batch_mean).abs() >= alpha * batch_std).float()\n",
    "\n",
    "\n",
    "def evaluate_ash_p(id_logits, ood_logits, replace_labels, alpha):\n",
    "    # Apply ASH-P to the ID and OOD logits\n",
    "    ash_p_id_logits = ash_p(id_logits, alpha)\n",
    "    ash_p_ood_logits = ash_p(ood_logits, alpha)\n",
    "\n",
    "    # Compute scores using the maximum logit value\n",
    "    id_scores = torch.max(ash_p_id_logits, dim=1)[0].cpu().numpy()\n",
    "    ood_scores = torch.max(ash_p_ood_logits, dim=1)[0].cpu().numpy()\n",
    "\n",
    "    # Combine scores and labels\n",
    "    combined_scores = np.concatenate([id_scores, ood_scores])\n",
    "\n",
    "    if replace_labels is None:\n",
    "        # Create labels (1 for ID, 0 for OOD)\n",
    "        id_labels = np.ones_like(id_scores)\n",
    "        ood_labels = np.zeros_like(ood_scores)\n",
    "\n",
    "        # Combine scores and labels\n",
    "        combined_labels = np.concatenate([id_labels, ood_labels])\n",
    "    else:\n",
    "        combined_labels = replace_labels\n",
    "\n",
    "    # Calculate AUROC\n",
    "    auroc = roc_auc_score(combined_labels, combined_scores)\n",
    "\n",
    "    # Calculate FPR at 95% TPR\n",
    "    fpr, tpr, thresholds = roc_curve(combined_labels, combined_scores)\n",
    "    fpr95 = fpr[np.where(tpr >= 0.95)[0][0]]\n",
    "\n",
    "    # Calculate Accuracy (assuming a threshold of 0.5 for binary classification)\n",
    "    predicted_labels = (combined_scores >= combined_scores.mean()).astype(int)\n",
    "    accuracy = accuracy_score(combined_labels, predicted_labels)\n",
    "\n",
    "    return auroc, fpr95, accuracy\n",
    "\n",
    "\n",
    "def evaluate_ash_s(id_logits, ood_logits, replace_labels, alpha):\n",
    "    # Apply ASH-S to the ID and OOD logits\n",
    "    ash_s_id_logits = ash_s(id_logits, alpha)\n",
    "    ash_s_ood_logits = ash_s(ood_logits, alpha)\n",
    "\n",
    "    # Compute scores using the maximum logit value\n",
    "    id_scores = torch.max(ash_s_id_logits, dim=1)[0].cpu().numpy()\n",
    "    ood_scores = torch.max(ash_s_ood_logits, dim=1)[0].cpu().numpy()\n",
    "\n",
    "    # Combine scores and labels\n",
    "    combined_scores = np.concatenate([id_scores, ood_scores])\n",
    "\n",
    "    if replace_labels is None:\n",
    "        # Create labels (1 for ID, 0 for OOD)\n",
    "        id_labels = np.ones_like(id_scores)\n",
    "        ood_labels = np.zeros_like(ood_scores)\n",
    "\n",
    "        # Combine scores and labels\n",
    "        combined_labels = np.concatenate([id_labels, ood_labels])\n",
    "    else:\n",
    "        combined_labels = replace_labels\n",
    "\n",
    "    # Calculate AUROC\n",
    "    auroc = roc_auc_score(combined_labels, combined_scores)\n",
    "\n",
    "    # Calculate FPR at 95% TPR\n",
    "    fpr, tpr, thresholds = roc_curve(combined_labels, combined_scores)\n",
    "    fpr95 = fpr[np.where(tpr >= 0.95)[0][0]]\n",
    "\n",
    "    # Calculate Accuracy (assuming a threshold of 0.5 for binary classification)\n",
    "    predicted_labels = (combined_scores >= combined_scores.mean()).astype(int)\n",
    "    accuracy = accuracy_score(combined_labels, predicted_labels)\n",
    "\n",
    "    return auroc, fpr95, accuracy\n",
    "\n",
    "\n",
    "def evaluate_ash_b(id_logits, ood_logits, replace_labels, alpha):\n",
    "    # Apply ASH-B to the ID and OOD logits\n",
    "    ash_b_id_logits = ash_b(id_logits, alpha)\n",
    "    ash_b_ood_logits = ash_b(ood_logits, alpha)\n",
    "\n",
    "    # Compute scores using the maximum logit value\n",
    "    id_scores = torch.max(ash_b_id_logits, dim=1)[0].cpu().numpy()\n",
    "    ood_scores = torch.max(ash_b_ood_logits, dim=1)[0].cpu().numpy()\n",
    "\n",
    "    # Combine scores and labels\n",
    "    combined_scores = np.concatenate([id_scores, ood_scores])\n",
    "\n",
    "    if replace_labels is None:\n",
    "        # Create labels (1 for ID, 0 for OOD)\n",
    "        id_labels = np.ones_like(id_scores)\n",
    "        ood_labels = np.zeros_like(ood_scores)\n",
    "\n",
    "        # Combine scores and labels\n",
    "        combined_labels = np.concatenate([id_labels, ood_labels])\n",
    "    else:\n",
    "        combined_labels = replace_labels\n",
    "\n",
    "    # Calculate AUROC\n",
    "    auroc = roc_auc_score(combined_labels, combined_scores)\n",
    "\n",
    "    # Calculate FPR at 95% TPR\n",
    "    fpr, tpr, thresholds = roc_curve(combined_labels, combined_scores)\n",
    "    fpr95 = fpr[np.where(tpr >= 0.95)[0][0]]\n",
    "\n",
    "    # Calculate Accuracy (assuming a threshold of 0.5 for binary classification)\n",
    "    predicted_labels = (combined_scores >= combined_scores.mean()).astype(int)\n",
    "    accuracy = accuracy_score(combined_labels, predicted_labels)\n",
    "\n",
    "    return auroc, fpr95, accuracy\n",
    "\n",
    "\n",
    "def find_optimal_threshold_react(\n",
    "    id_logits, ood_logits, threshold_range, replace_labels, metric=\"auroc\"\n",
    "):\n",
    "    best_threshold = None\n",
    "    best_metric_value = (\n",
    "        -float(\"inf\") if metric in [\"auroc\", \"accuracy\"] else float(\"inf\")\n",
    "    )\n",
    "\n",
    "    for threshold in threshold_range:\n",
    "        auroc, fpr95, accuracy = evaluate_react(\n",
    "            id_logits, ood_logits, replace_labels, threshold\n",
    "        )\n",
    "        print(\n",
    "            f\"Threshold: {threshold}, AUROC: {auroc}, FPR95: {fpr95}, Accuracy: {accuracy}\"\n",
    "        )\n",
    "\n",
    "        if metric == \"auroc\":\n",
    "            current_metric_value = auroc\n",
    "        elif metric == \"fpr95\":\n",
    "            current_metric_value = fpr95\n",
    "        elif metric == \"accuracy\":\n",
    "            current_metric_value = accuracy\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid metric specified. Choose from 'auroc', 'fpr95', or 'accuracy'.\"\n",
    "            )\n",
    "\n",
    "        if (\n",
    "            metric in [\"auroc\", \"accuracy\"] and current_metric_value > best_metric_value\n",
    "        ) or (metric == \"fpr95\" and current_metric_value < best_metric_value):\n",
    "            best_metric_value = current_metric_value\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print(\n",
    "        f\"Optimal Threshold: {best_threshold} with {metric.upper()}: {best_metric_value}\"\n",
    "    )\n",
    "    return best_threshold\n",
    "\n",
    "\n",
    "def find_optimal_alpha_ash_p(\n",
    "    id_logits, ood_logits, alpha_range, replace_labels, metric=\"auroc\"\n",
    "):\n",
    "    best_alpha = None\n",
    "    best_metric_value = (\n",
    "        -float(\"inf\") if metric in [\"auroc\", \"accuracy\"] else float(\"inf\")\n",
    "    )\n",
    "\n",
    "    for alpha in alpha_range:\n",
    "        auroc, fpr95, accuracy = evaluate_ash_p(\n",
    "            id_logits, ood_logits, replace_labels, alpha\n",
    "        )\n",
    "        print(f\"Alpha: {alpha}, AUROC: {auroc}, FPR95: {fpr95}, Accuracy: {accuracy}\")\n",
    "\n",
    "        if metric == \"auroc\":\n",
    "            current_metric_value = auroc\n",
    "        elif metric == \"fpr95\":\n",
    "            current_metric_value = fpr95\n",
    "        elif metric == \"accuracy\":\n",
    "            current_metric_value = accuracy\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid metric specified. Choose from 'auroc', 'fpr95', or 'accuracy'.\"\n",
    "            )\n",
    "\n",
    "        if (\n",
    "            metric in [\"auroc\", \"accuracy\"] and current_metric_value > best_metric_value\n",
    "        ) or (metric == \"fpr95\" and current_metric_value < best_metric_value):\n",
    "            best_metric_value = current_metric_value\n",
    "            best_alpha = alpha\n",
    "\n",
    "    print(f\"Optimal Alpha: {best_alpha} with {metric.upper()}: {best_metric_value}\")\n",
    "    return best_alpha\n",
    "\n",
    "\n",
    "def find_optimal_alpha_ash_b(\n",
    "    id_logits, ood_logits, alpha_range, replace_labels, metric=\"auroc\"\n",
    "):\n",
    "    best_alpha = None\n",
    "    best_metric_value = (\n",
    "        -float(\"inf\") if metric in [\"auroc\", \"accuracy\"] else float(\"inf\")\n",
    "    )\n",
    "\n",
    "    for alpha in alpha_range:\n",
    "        auroc, fpr95, accuracy = evaluate_ash_b(\n",
    "            id_logits, ood_logits, replace_labels, alpha\n",
    "        )\n",
    "        print(f\"Alpha: {alpha}, AUROC: {auroc}, FPR95: {fpr95}, Accuracy: {accuracy}\")\n",
    "\n",
    "        if metric == \"auroc\":\n",
    "            current_metric_value = auroc\n",
    "        elif metric == \"fpr95\":\n",
    "            current_metric_value = fpr95\n",
    "        elif metric == \"accuracy\":\n",
    "            current_metric_value = accuracy\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid metric specified. Choose from 'auroc', 'fpr95', or 'accuracy'.\"\n",
    "            )\n",
    "\n",
    "        if (\n",
    "            metric in [\"auroc\", \"accuracy\"] and current_metric_value > best_metric_value\n",
    "        ) or (metric == \"fpr95\" and current_metric_value < best_metric_value):\n",
    "            best_metric_value = current_metric_value\n",
    "            best_alpha = alpha\n",
    "\n",
    "    print(f\"Optimal Alpha: {best_alpha} with {metric.upper()}: {best_metric_value}\")\n",
    "    return best_alpha\n",
    "\n",
    "\n",
    "def find_optimal_alpha_ash_s(\n",
    "    id_logits, ood_logits, alpha_range, replace_labels, metric=\"auroc\"\n",
    "):\n",
    "    best_alpha = None\n",
    "    best_metric_value = (\n",
    "        -float(\"inf\") if metric in [\"auroc\", \"accuracy\"] else float(\"inf\")\n",
    "    )\n",
    "\n",
    "    for alpha in alpha_range:\n",
    "        auroc, fpr95, accuracy = evaluate_ash_s(\n",
    "            id_logits, ood_logits, replace_labels, alpha\n",
    "        )\n",
    "        print(f\"Alpha: {alpha}, AUROC: {auroc}, FPR95: {fpr95}, Accuracy: {accuracy}\")\n",
    "\n",
    "        if metric == \"auroc\":\n",
    "            current_metric_value = auroc\n",
    "        elif metric == \"fpr95\":\n",
    "            current_metric_value = fpr95\n",
    "        elif metric == \"accuracy\":\n",
    "            current_metric_value = accuracy\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid metric specified. Choose from 'auroc', 'fpr95', or 'accuracy'.\"\n",
    "            )\n",
    "\n",
    "        if (\n",
    "            metric in [\"auroc\", \"accuracy\"] and current_metric_value > best_metric_value\n",
    "        ) or (metric == \"fpr95\" and current_metric_value < best_metric_value):\n",
    "            best_metric_value = current_metric_value\n",
    "            best_alpha = alpha\n",
    "\n",
    "    print(f\"Optimal Alpha: {best_alpha} with {metric.upper()}: {best_metric_value}\")\n",
    "    return best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observe_metric_search = \"accuracy\"\n",
    "results = {}\n",
    "alpha_range = np.linspace(0.1, 10, num=100)\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "labels = {\"gt\": None, \"surr\": y_surrogate_combined_labels, \"rand\": y_random}\n",
    "\n",
    "for key, value in labels.items():\n",
    "    replace_labels = value\n",
    "    print(f\"Replace Labels: {key}\")\n",
    "    optimal_alpha_p_ash_p = find_optimal_alpha_ash_p(\n",
    "        X_train_tensor,\n",
    "        X_test_tensor,\n",
    "        alpha_range,\n",
    "        replace_labels=replace_labels,\n",
    "        metric=observe_metric_search,\n",
    "    )\n",
    "    auroc, fpr95, accuracy = evaluate_ash_p(\n",
    "        X_train_tensor,\n",
    "        X_test_tensor,\n",
    "        replace_labels=replace_labels,\n",
    "        alpha=optimal_alpha_p_ash_p,\n",
    "    )\n",
    "    print(f\"AUROC: {auroc}, FPR95: {fpr95}, Accuracy: {accuracy}\")\n",
    "\n",
    "    results[key] = {\"Accuracy\": accuracy}\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del results, fpr95, auroc, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observe_metric_search = \"accuracy\"\n",
    "results = {}\n",
    "alpha_range = np.linspace(0.1, 10, num=100)\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "labels = {\"gt\": None, \"surr\": y_surrogate_combined_labels, \"rand\": y_random}\n",
    "\n",
    "for key, value in labels.items():\n",
    "    replace_labels = value\n",
    "    print(f\"Replace Labels: {key}\")\n",
    "    optimal_alpha_s_ash_s = find_optimal_alpha_ash_s(\n",
    "        X_train_tensor,\n",
    "        X_test_tensor,\n",
    "        alpha_range,\n",
    "        replace_labels=replace_labels,\n",
    "        metric=observe_metric_search,\n",
    "    )\n",
    "    auroc, fpr95, accuracy = evaluate_ash_s(\n",
    "        X_train_tensor,\n",
    "        X_test_tensor,\n",
    "        replace_labels=replace_labels,\n",
    "        alpha=optimal_alpha_s_ash_s,\n",
    "    )\n",
    "    print(f\"AUROC: {auroc}, FPR95: {fpr95}, Accuracy: {accuracy}\")\n",
    "\n",
    "    results[key] = {\"Accuracy\": accuracy}\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need input images, not samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ODIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting ID to 1000 samples due to OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "\n",
    "# ODIN Function\n",
    "def odin(model, inputs, temperature, epsilon):\n",
    "    inputs = inputs\n",
    "    inputs.requires_grad = True\n",
    "    outputs = model(inputs)\n",
    "    # print(outputs)\n",
    "    outputs = outputs / temperature\n",
    "\n",
    "    max_logit = torch.max(outputs, dim=1)[0]\n",
    "    max_logit.backward(torch.ones_like(max_logit))\n",
    "\n",
    "    perturbation = epsilon * inputs.grad.sign()\n",
    "    # print(perturbation)\n",
    "    # print(inputs)\n",
    "    perturbed_inputs = inputs + perturbation\n",
    "    perturbed_inputs = torch.clamp(perturbed_inputs, 0, 1)\n",
    "\n",
    "    perturbed_outputs = model(perturbed_inputs)\n",
    "    perturbed_outputs = perturbed_outputs / temperature\n",
    "\n",
    "    return perturbed_outputs\n",
    "\n",
    "\n",
    "# Evaluation Function for ODIN\n",
    "def evaluate_odin(model, id_inputs, ood_inputs, temperature, replace_labels, epsilon):\n",
    "    odin_id_logits = odin(model, id_inputs, temperature, epsilon)\n",
    "    odin_ood_logits = odin(model, ood_inputs, temperature, epsilon)\n",
    "\n",
    "    id_scores = torch.max(odin_id_logits, dim=1)[0].detach().cpu().numpy()\n",
    "    ood_scores = torch.max(odin_ood_logits, dim=1)[0].detach().cpu().numpy()\n",
    "\n",
    "    combined_scores = np.concatenate([id_scores, ood_scores])\n",
    "\n",
    "    if replace_labels is None:\n",
    "        # Create labels (1 for ID, 0 for OOD)\n",
    "        id_labels = np.ones_like(id_scores)\n",
    "        ood_labels = np.zeros_like(ood_scores)\n",
    "\n",
    "        # Combine scores and labels\n",
    "        combined_labels = np.concatenate([id_labels, ood_labels])\n",
    "    else:\n",
    "        combined_labels = replace_labels\n",
    "\n",
    "    auroc = roc_auc_score(combined_labels, combined_scores)\n",
    "    fpr, tpr, thresholds = roc_curve(combined_labels, combined_scores)\n",
    "    fpr95 = fpr[np.where(tpr >= 0.95)[0][0]]\n",
    "    predicted_labels = (combined_scores >= 0.5).astype(int)\n",
    "    accuracy = accuracy_score(combined_labels, predicted_labels)\n",
    "\n",
    "    return auroc, fpr95, accuracy\n",
    "\n",
    "\n",
    "# Find Optimal Temperature and Epsilon for ODIN\n",
    "def find_optimal_odin_params(\n",
    "    model,\n",
    "    id_inputs,\n",
    "    ood_inputs,\n",
    "    temperature_range,\n",
    "    epsilon_range,\n",
    "    replace_labels,\n",
    "    metric=\"auroc\",\n",
    "):\n",
    "    best_temperature = None\n",
    "    best_epsilon = None\n",
    "    best_metric_value = (\n",
    "        -float(\"inf\") if metric in [\"auroc\", \"accuracy\"] else float(\"inf\")\n",
    "    )\n",
    "\n",
    "    for temperature in temperature_range:\n",
    "        for epsilon in epsilon_range:\n",
    "            auroc, fpr95, accuracy = evaluate_odin(\n",
    "                model, id_inputs, ood_inputs, temperature, replace_labels, epsilon\n",
    "            )\n",
    "            print(\n",
    "                f\"Temperature: {temperature}, Epsilon: {epsilon}, AUROC: {auroc}, FPR95: {fpr95}, Accuracy: {accuracy}\"\n",
    "            )\n",
    "\n",
    "            if metric == \"auroc\":\n",
    "                current_metric_value = auroc\n",
    "            elif metric == \"fpr95\":\n",
    "                current_metric_value = fpr95\n",
    "            elif metric == \"accuracy\":\n",
    "                current_metric_value = accuracy\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Invalid metric specified. Choose from 'auroc', 'fpr95', or 'accuracy'.\"\n",
    "                )\n",
    "\n",
    "            if (\n",
    "                metric in [\"auroc\", \"accuracy\"]\n",
    "                and current_metric_value > best_metric_value\n",
    "            ) or (metric == \"fpr95\" and current_metric_value < best_metric_value):\n",
    "                best_metric_value = current_metric_value\n",
    "                best_temperature = temperature\n",
    "                best_epsilon = epsilon\n",
    "\n",
    "    print(\n",
    "        f\"Optimal Temperature: {best_temperature}, Optimal Epsilon: {best_epsilon} with {metric.upper()}: {best_metric_value}\"\n",
    "    )\n",
    "    return best_temperature, best_epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I set different temperature and epsilon for ID and OOD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_range = np.linspace(1.0, 10.0, num=10)\n",
    "epsilon_range = np.linspace(0.0, 0.1, num=10)\n",
    "\n",
    "monitor_metric = \"accuracy\"\n",
    "results = {}\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "labels = {\"gt\": None, \"surr\": y_surrogate_combined_labels, \"rand\": y_random}\n",
    "\n",
    "for key, value in labels.items():\n",
    "    replace_labels = value\n",
    "    print(f\"Replace Labels: {key}\")\n",
    "\n",
    "    # Find the optimal temperature and epsilon for ODIN based on AUROC\n",
    "    optimal_temperature, optimal_epsilon = find_optimal_odin_params(\n",
    "        model,\n",
    "        x_sample_train_tensor,\n",
    "        x_sample_test_tensor,\n",
    "        temperature_range,\n",
    "        epsilon_range,\n",
    "        replace_labels=replace_labels,\n",
    "        metric=monitor_metric,\n",
    "    )\n",
    "    #  2.0 0.03333333333333333\n",
    "    # Evaluate ODIN with the optimal temperature and epsilon\n",
    "    auroc_odin, fpr95_odin, accuracy_odin = evaluate_odin(\n",
    "        model,\n",
    "        x_sample_train_tensor,\n",
    "        x_sample_test_tensor,\n",
    "        optimal_temperature,\n",
    "        replace_labels=replace_labels,\n",
    "        optimal_epsilon=optimal_epsilon,\n",
    "    )\n",
    "    print(f\"ODIN - AUROC: {auroc_odin}, FPR95: {fpr95_odin}, Accuracy: {accuracy_odin}\")\n",
    "\n",
    "    results[key] = {\n",
    "        \"AUROC\": auroc_odin,\n",
    "        \"FPR at 95% TPR\": fpr95_odin,\n",
    "        \"Accuracy\": accuracy_odin,\n",
    "    }\n",
    "\n",
    "    results[key] = {\"Accuracy\": accuracy}\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSP, Energy, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "# Define all the evaluation functions\n",
    "\n",
    "\n",
    "# Maximum Softmax Probability (MSP)\n",
    "def evaluate_msp(model, id_inputs, ood_inputs):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        id_outputs = model(id_inputs)\n",
    "        ood_outputs = model(ood_inputs)\n",
    "\n",
    "        id_scores = torch.max(torch.softmax(id_outputs, dim=1), dim=1)[0].cpu().numpy()\n",
    "        ood_scores = (\n",
    "            torch.max(torch.softmax(ood_outputs, dim=1), dim=1)[0].cpu().numpy()\n",
    "        )\n",
    "\n",
    "    # Combine scores and labels\n",
    "    combined_scores = np.concatenate([id_scores, ood_scores])\n",
    "\n",
    "    if replace_labels is None:\n",
    "        # Create labels (1 for ID, 0 for OOD)\n",
    "        id_labels = np.ones_like(id_scores)\n",
    "        ood_labels = np.zeros_like(ood_scores)\n",
    "\n",
    "        # Combine scores and labels\n",
    "        combined_labels = np.concatenate([id_labels, ood_labels])\n",
    "    else:\n",
    "        combined_labels = r\n",
    "\n",
    "    auroc = roc_auc_score(combined_labels, combined_scores)\n",
    "    fpr, tpr, thresholds = roc_curve(combined_labels, combined_scores)\n",
    "    fpr95 = fpr[np.where(tpr >= 0.95)[0][0]]\n",
    "    predicted_labels = (combined_scores >= combined_scores.mean()).astype(int)\n",
    "    accuracy = accuracy_score(combined_labels, predicted_labels)\n",
    "\n",
    "    return auroc, fpr95, accuracy\n",
    "\n",
    "\n",
    "def _get_energy_score(logits, temperature=2):\n",
    "    scores = -(temperature * torch.logsumexp(logits / temperature, dim=1)).cpu().numpy()\n",
    "    return scores\n",
    "\n",
    "\n",
    "def evaluate_energy(model, id_inputs, ood_inputs, temperature=2):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        id_outputs = model(id_inputs)\n",
    "        ood_outputs = model(ood_inputs)\n",
    "\n",
    "        id_scores = _get_energy_score(id_outputs, temperature)\n",
    "        ood_scores = _get_energy_score(ood_outputs, temperature)\n",
    "\n",
    "    id_labels = np.ones_like(id_scores)\n",
    "    ood_labels = np.zeros_like(ood_scores)\n",
    "\n",
    "    combined_scores = np.concatenate([id_scores, ood_scores])\n",
    "    combined_labels = np.concatenate([id_labels, ood_labels])\n",
    "\n",
    "    auroc = roc_auc_score(combined_labels, combined_scores)\n",
    "    fpr, tpr, thresholds = roc_curve(combined_labels, combined_scores)\n",
    "    fpr95 = fpr[np.where(tpr >= 0.95)[0][0]]\n",
    "    # print(combined_scores.min(), combined_scores.max(), combined_scores.mean()) ###########\n",
    "    predicted_labels = (combined_scores >= combined_scores.mean()).astype(int)\n",
    "    accuracy = accuracy_score(combined_labels, predicted_labels)\n",
    "\n",
    "    return auroc, fpr95, accuracy\n",
    "\n",
    "\n",
    "def find_optimal_temperature(\n",
    "    model, id_inputs, ood_inputs, temperature_range, metric=\"auroc\"\n",
    "):\n",
    "    best_temperature = None\n",
    "    best_metric_value = (\n",
    "        -float(\"inf\") if metric in [\"auroc\", \"accuracy\"] else float(\"inf\")\n",
    "    )\n",
    "\n",
    "    for temperature in temperature_range:\n",
    "        auroc, fpr95, accuracy = evaluate_energy(\n",
    "            model, id_inputs, ood_inputs, temperature\n",
    "        )\n",
    "        print(\n",
    "            f\"Temperature: {temperature}, AUROC: {auroc}, FPR95: {fpr95}, Accuracy: {accuracy}\"\n",
    "        )\n",
    "\n",
    "        if metric == \"auroc\":\n",
    "            current_metric_value = auroc\n",
    "        elif metric == \"fpr95\":\n",
    "            current_metric_value = fpr95\n",
    "        elif metric == \"accuracy\":\n",
    "            current_metric_value = accuracy\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid metric specified. Choose from 'auroc', 'fpr95', or 'accuracy'.\"\n",
    "            )\n",
    "\n",
    "        if (\n",
    "            metric in [\"auroc\", \"accuracy\"] and current_metric_value > best_metric_value\n",
    "        ) or (metric == \"fpr95\" and current_metric_value < best_metric_value):\n",
    "            best_metric_value = current_metric_value\n",
    "            best_temperature = temperature\n",
    "\n",
    "    print(\n",
    "        f\"Optimal Temperature: {best_temperature} with {metric.upper()}: {best_metric_value}\"\n",
    "    )\n",
    "    return best_temperature\n",
    "\n",
    "\n",
    "# Softmax Method\n",
    "def evaluate_softmax(model, id_inputs, ood_inputs):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        id_outputs = model(id_inputs)\n",
    "        ood_outputs = model(ood_inputs)\n",
    "\n",
    "        id_softmax = torch.softmax(id_outputs, dim=1).cpu().numpy()\n",
    "        ood_softmax = torch.softmax(ood_outputs, dim=1).cpu().numpy()\n",
    "\n",
    "    id_labels = np.ones(len(id_softmax))\n",
    "    ood_labels = np.zeros(len(ood_softmax))\n",
    "\n",
    "    combined_softmax = np.concatenate([id_softmax, ood_softmax])\n",
    "    combined_labels = np.concatenate([id_labels, ood_labels])\n",
    "\n",
    "    # Calculate AUROC for each class and average\n",
    "    auroc_per_class = []\n",
    "    for i in range(combined_softmax.shape[1]):\n",
    "        auroc = roc_auc_score(combined_labels, combined_softmax[:, i])\n",
    "        auroc_per_class.append(auroc)\n",
    "\n",
    "    mean_auroc = np.mean(auroc_per_class)\n",
    "\n",
    "    # Use the max softmax probability for FPR95 and accuracy\n",
    "    combined_scores = np.max(combined_softmax, axis=1)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(combined_labels, combined_scores)\n",
    "    fpr95 = fpr[np.where(tpr >= 0.95)[0][0]]\n",
    "    predicted_labels = (combined_scores >= combined_scores.mean()).astype(int)\n",
    "    accuracy = accuracy_score(combined_labels, predicted_labels)\n",
    "\n",
    "    return mean_auroc, fpr95, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "id_inputs_ = id_inputs  # _subset\n",
    "ood_inputs_ = ood_inputs\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Evaluate MSP\n",
    "auroc_msp, fpr95_msp, accuracy_msp = evaluate_msp(model, id_inputs_, ood_inputs_)\n",
    "print(f\"MSP - AUROC: {auroc_msp}, FPR95: {fpr95_msp}, Accuracy: {accuracy_msp}\")\n",
    "results[\"MSP\"] = {\n",
    "    \"AUROC\": auroc_msp,\n",
    "    \"FPR at 95% TPR\": fpr95_msp,\n",
    "    \"Accuracy\": accuracy_msp,\n",
    "}\n",
    "\n",
    "# Define the range of temperatures to search\n",
    "# temperature_range = np.linspace(1, 50.0, num=10)\n",
    "# Find the optimal temperature based on AUROC\n",
    "# Evaluate Energy with the optimal temperature\n",
    "# optimal_temperature = find_optimal_temperature(model, id_inputs, ood_inputs, temperature_range, metric='accuracy')\n",
    "auroc_energy, fpr95_energy, accuracy_energy = evaluate_energy(\n",
    "    model, id_inputs_, ood_inputs_, temperature=39\n",
    ")\n",
    "print(\n",
    "    f\"Energy - AUROC: {auroc_energy}, FPR95: {fpr95_energy}, Accuracy: {accuracy_energy}\"\n",
    ")\n",
    "\n",
    "# Evaluate Softmax Method\n",
    "auroc_softmax, fpr95_softmax, accuracy_softmax = evaluate_softmax(\n",
    "    model, id_inputs_, ood_inputs_\n",
    ")\n",
    "print(\n",
    "    f\"Softmax - AUROC: {auroc_softmax}, FPR95: {fpr95_softmax}, Accuracy: {accuracy_softmax}\"\n",
    ")\n",
    "results[\"Softmax\"] = {\n",
    "    \"AUROC\": auroc_softmax,\n",
    "    \"FPR at 95% TPR\": fpr95_softmax,\n",
    "    \"Accuracy\": accuracy_softmax,\n",
    "}\n",
    "\n",
    "print(\"-----------results\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layer_names = get_all_layer_names(model)\n",
    "\n",
    "\n",
    "def pick_random_layers(layers, n):\n",
    "    first_conv = conv_layers[0]  # First conv layer\n",
    "    last_conv = conv_layers[-1]  # Last conv layer\n",
    "    middle_layers = conv_layers[1:-1]\n",
    "    random_layers = random.sample(middle_layers, min(n - 2, len(middle_layers)))\n",
    "    selected_layers = [first_conv] + random_layers + [last_conv]\n",
    "    return selected_layers\n",
    "\n",
    "\n",
    "conv_layers = [layer for layer in all_layer_names if \"conv\" in layer]\n",
    "selected_layers = pick_random_layers(all_layer_names, 10)\n",
    "selected_layers = [[layer] for layer in selected_layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g\", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k_condidence_g_hat(\n",
    "    X,\n",
    "    y,\n",
    "    M,\n",
    "    test_size=0.2,\n",
    "    n_runs=10,\n",
    "    confidence_level=0.95,\n",
    "    confidence_intervals_g=confidence_intervals_g,\n",
    "    clf=None,\n",
    "    save_plot=True,\n",
    "    fname=cfg_dict[\"data\"][\"class_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster,\n",
    "    y_train_cluster,\n",
    "    y_clusters,\n",
    "    class_name=cfg_dict[\"data\"][\"class_name\"],\n",
    "    save_plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HerbaceousVegetation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, model, datamodule, train_dataloader, val_dataloader, test_dataloader, cfg_dict = (\n",
    "    get_X_y_arrays(\n",
    "        paths[\"herb_veg\"][\"config\"],\n",
    "        paths[\"herb_veg\"][\"ckpt\"],\n",
    "        layer,\n",
    "        downsample_method,\n",
    "        getitem_keys,\n",
    "        device,\n",
    "        n_batches_to_process,\n",
    "        mode=\"holdout\",\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g: \", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k_condidence_g_hat(\n",
    "    X,\n",
    "    y,\n",
    "    M,\n",
    "    test_size=0.2,\n",
    "    n_runs=10,\n",
    "    confidence_level=0.95,\n",
    "    confidence_intervals_g=confidence_intervals_g,\n",
    "    clf=None,\n",
    "    save_plot=True,\n",
    "    fname=cfg_dict[\"data\"][\"class_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster,\n",
    "    y_train_cluster,\n",
    "    y_clusters,\n",
    "    class_name=cfg_dict.data.class_name,\n",
    "    save_plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, model, datamodule, train_dataloader, val_dataloader, test_dataloader, cfg_dict = (\n",
    "    get_X_y_arrays(\n",
    "        paths[\"highway\"][\"config\"],\n",
    "        paths[\"highway\"][\"ckpt\"],\n",
    "        layer,\n",
    "        downsample_method,\n",
    "        getitem_keys,\n",
    "        device,\n",
    "        n_batches_to_process,\n",
    "        mode=\"holdout\",\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g: \", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k_condidence_g_hat(\n",
    "    X,\n",
    "    y,\n",
    "    M,\n",
    "    test_size=0.2,\n",
    "    n_runs=10,\n",
    "    confidence_level=0.95,\n",
    "    confidence_intervals_g=confidence_intervals_g,\n",
    "    clf=None,\n",
    "    save_plot=True,\n",
    "    fname=cfg_dict[\"data\"][\"class_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster,\n",
    "    y_train_cluster,\n",
    "    y_clusters,\n",
    "    class_name=cfg_dict.data.class_name,\n",
    "    save_plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industrial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, model, datamodule, train_dataloader, val_dataloader, test_dataloader, cfg_dict = (\n",
    "    get_X_y_arrays(\n",
    "        paths[\"industrial\"][\"config\"],\n",
    "        paths[\"industrial\"][\"ckpt\"],\n",
    "        layer,\n",
    "        downsample_method,\n",
    "        getitem_keys,\n",
    "        device,\n",
    "        n_batches_to_process,\n",
    "        mode=\"holdout\",\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g: \", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k_condidence_g_hat(\n",
    "    X,\n",
    "    y,\n",
    "    M,\n",
    "    test_size=0.2,\n",
    "    n_runs=10,\n",
    "    confidence_level=0.95,\n",
    "    confidence_intervals_g=confidence_intervals_g,\n",
    "    clf=None,\n",
    "    save_plot=True,\n",
    "    fname=cfg_dict[\"data\"][\"class_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster,\n",
    "    y_train_cluster,\n",
    "    y_clusters,\n",
    "    class_name=cfg_dict.data.class_name,\n",
    "    save_plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, model, datamodule, train_dataloader, val_dataloader, test_dataloader, cfg_dict = (\n",
    "    get_X_y_arrays(\n",
    "        paths[\"pasture\"][\"config\"],\n",
    "        paths[\"pasture\"][\"ckpt\"],\n",
    "        layer,\n",
    "        downsample_method,\n",
    "        getitem_keys,\n",
    "        device,\n",
    "        n_batches_to_process,\n",
    "        mode=\"holdout\",\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g\", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k_condidence_g_hat(\n",
    "    X,\n",
    "    y,\n",
    "    M,\n",
    "    test_size=0.2,\n",
    "    n_runs=10,\n",
    "    confidence_level=0.95,\n",
    "    confidence_intervals_g=confidence_intervals_g,\n",
    "    clf=None,\n",
    "    save_plot=True,\n",
    "    fname=cfg_dict[\"data\"][\"class_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster,\n",
    "    y_train_cluster,\n",
    "    y_clusters,\n",
    "    class_name=cfg_dict[\"data\"][\"class_name\"],\n",
    "    save_plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permanent Crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, model, datamodule, train_dataloader, val_dataloader, test_dataloader, cfg_dict = (\n",
    "    get_X_y_arrays(\n",
    "        paths[\"permanentcrop\"][\"config\"],\n",
    "        paths[\"permanentcrop\"][\"ckpt\"],\n",
    "        layer,\n",
    "        downsample_method,\n",
    "        getitem_keys,\n",
    "        device,\n",
    "        n_batches_to_process,\n",
    "        mode=\"holdout\",\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g: \", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k(\n",
    "    X, y, M, confidence_intervals_g=confidence_intervals_g\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster, y_train_cluster, y_clusters, class_name=cfg.data.class_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, model, datamodule, train_dataloader, val_dataloader, test_dataloader, cfg_dict = (\n",
    "    get_X_y_arrays(\n",
    "        paths[\"residential\"][\"config\"],\n",
    "        paths[\"residential\"][\"ckpt\"],\n",
    "        layer,\n",
    "        downsample_method,\n",
    "        getitem_keys,\n",
    "        device,\n",
    "        n_batches_to_process,\n",
    "        mode=\"holdout\",\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g: \", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k(\n",
    "    X, y, M, confidence_intervals_g=confidence_intervals_g\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster, y_train_cluster, y_clusters, class_name=cfg.data.class_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# River"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, model, datamodule, train_dataloader, val_dataloader, test_dataloader, cfg_dict = (\n",
    "    get_X_y_arrays(\n",
    "        paths[\"river\"][\"config\"],\n",
    "        paths[\"river\"][\"ckpt\"],\n",
    "        layer,\n",
    "        downsample_method,\n",
    "        getitem_keys,\n",
    "        device,\n",
    "        n_batches_to_process,\n",
    "        mode=\"holdout\",\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g: \", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k(\n",
    "    X, y, M, confidence_intervals_g=confidence_intervals_g\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster, y_train_cluster, y_clusters, class_name=cfg.data.class_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeaLake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, model, datamodule, train_dataloader, val_dataloader, test_dataloader, cfg_dict = (\n",
    "    get_X_y_arrays(\n",
    "        paths[\"sealake\"][\"config\"],\n",
    "        paths[\"sealake\"][\"ckpt\"],\n",
    "        layer,\n",
    "        downsample_method,\n",
    "        getitem_keys,\n",
    "        device,\n",
    "        n_batches_to_process,\n",
    "        mode=\"holdout\",\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g: \", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k(\n",
    "    X, y, M, confidence_intervals_g=confidence_intervals_g\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster, y_train_cluster, y_clusters, class_name=cfg.data.class_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annual Crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, model, datamodule, train_dataloader, val_dataloader, test_dataloader, cfg_dict = (\n",
    "    get_X_y_arrays(\n",
    "        paths[\"annualcrop\"][\"config\"],\n",
    "        paths[\"annualcrop\"][\"ckpt\"],\n",
    "        layer,\n",
    "        downsample_method,\n",
    "        getitem_keys,\n",
    "        device,\n",
    "        n_batches_to_process,\n",
    "        mode=\"holdout\",\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g\", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k_condidence_g_hat(\n",
    "    X,\n",
    "    y,\n",
    "    M,\n",
    "    test_size=0.2,\n",
    "    n_runs=10,\n",
    "    confidence_level=0.95,\n",
    "    confidence_intervals_g=confidence_intervals_g,\n",
    "    clf=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster, y_train_cluster, y_clusters, class_name=cfg.data.class_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.__class__.__name__ + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.__class__.__name__ + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g: \", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k(\n",
    "    X, y, M, confidence_intervals_g=confidence_intervals_g\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster, y_train_cluster, y_clusters, class_name=cfg.data.class_name\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ood",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
