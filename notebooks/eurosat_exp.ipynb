{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Microsoft Corporation.\n",
    "\n",
    "Licensed under the MIT License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with EuroSAT dataset under 11 settings:\n",
    "- Semantic Shift: Leave-one-class-out. Train on 9 class and test on 10 class \n",
    "- Covariate Shift: Longitude-wise split. Train on West and test on East \n",
    "\n",
    "For each, we perform: \n",
    "- activation extraction\n",
    "- downsample benchmarking\n",
    "- layer benchmarking\n",
    "- g training and evaluation\n",
    "- g_hat training and evaluation\n",
    "- g and g_hat statistical significance test\n",
    "- g benchmark\n",
    "- clustering benchmark\n",
    "- num_cluster vs. g and g_hat performance investigation\n",
    "- activation space visualization    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 2\n",
      "Current GPU: 0\n",
      "GPU Name: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.tardis.eurosat_xbd_utils import *\n",
    "from src.tardis.utils import *\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"Current GPU:\", torch.cuda.current_device())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "fixed_seed = 31\n",
    "set_seed(fixed_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main directories for checkpoints and configs\n",
    "ckpt_main_dir = \"./exp_data/main_tardis/eurosat_exp_logs\"\n",
    "config_main_dir = \"./geospatial-ood-detection/configs\"\n",
    "\n",
    "# Define a function to construct the paths dynamically\n",
    "def construct_paths(main_dir, sub_dir, filename):\n",
    "    return os.path.join(main_dir, sub_dir, filename)\n",
    "\n",
    "# Define subdirectories and filenames for each configuration and checkpoint\n",
    "paths = {\n",
    "    \"forest\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_forest.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir, \"Holdout_Forest_resnet50_0066\", \"epoch=21-step=4972.ckpt\"\n",
    "        ),\n",
    "    },\n",
    "    \"herb_veg\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_herbaceousvegetation.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir,\n",
    "            \"Holdout_HerbaceousVegetation_resnet50_0066\",\n",
    "            \"epoch=27-step=6328.ckpt\",\n",
    "        ),\n",
    "    },\n",
    "    \"highway\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_highway.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir, \"Holdout_Highway_resnet50_0066\", \"epoch=28-step=6670.ckpt\"\n",
    "        ),\n",
    "    },\n",
    "    \"industrial\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_industrial.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir, \"Holdout_Industrial_resnet50_0066\", \"epoch=32-step=7590.ckpt\"\n",
    "        ),\n",
    "    },\n",
    "    \"pasture\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_pasture.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir, \"Holdout_Pasture_resnet50_0066\", \"epoch=34-step=8225.ckpt\"\n",
    "        ),\n",
    "    },\n",
    "    \"permanentcrop\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_permanentcrop.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir,\n",
    "            \"Holdout_PermanentCrop_resnet50_0066\",\n",
    "            \"epoch=26-step=6210.ckpt\",\n",
    "        ),\n",
    "    },\n",
    "    \"residential\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_residential.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir,\n",
    "            \"Holdout_Residential_resnet50_0066\",\n",
    "            \"epoch=49-step=11250.ckpt\",\n",
    "        ),\n",
    "    },\n",
    "    \"river\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_river.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir, \"Holdout_River_resnet50_0066\", \"epoch=31-step=7392.ckpt\"\n",
    "        ),\n",
    "    },\n",
    "    \"sealake\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_sealake.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir, \"Holdout_SeaLake_resnet50_0066\", \"epoch=34-step=7875.ckpt\"\n",
    "        ),\n",
    "    },\n",
    "    \"annualcrop\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_holdout_annualcrop.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir, \"Holdout_AnnualCrop_resnet50_0066\", \"epoch=15-step=3616.ckpt\"\n",
    "        ),\n",
    "    },\n",
    "    \"spatial_split\": {\n",
    "        \"config\": construct_paths(\n",
    "            config_main_dir, \"eurosat\", \"eurosat_spatial_config.yaml\"\n",
    "        ),\n",
    "        \"ckpt\": construct_paths(\n",
    "            ckpt_main_dir, \"eurosat_spatial_0776\", \"epoch=18-step=4826.ckpt\"\n",
    "        ),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define other parameters\n",
    "layer = [\"conv1\"]\n",
    "downsample_method = \"avg_pool\"\n",
    "getitem_keys = [\"image\", \"label\"]\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_batches_to_process = 2\n",
    "\n",
    "# Downsampling methods and benchmarks\n",
    "downsample_methods = [\"avg_pool\", \"mean_std\", \"avg_pool\", \"max_pool\", \"nodownsample\"]\n",
    "downsample_benchmark = {}\n",
    "layer_benchmark = {}\n",
    "\n",
    "collect_activations_from_layers = [\"conv1\"]\n",
    "getitem_keys = [\"image\", \"label\"]\n",
    "verbose = False\n",
    "test_size = 0.2\n",
    "n_estimators = 100\n",
    "split_seed = 31\n",
    "fixed_classifier_seed = 31\n",
    "\n",
    "n_optuna_trials = 20\n",
    "min_cluster = 2\n",
    "max_cluster_ratio = 0.3\n",
    "min_fraction = 0.01\n",
    "max_fraction = 0.2\n",
    "fixed_seed = 31\n",
    "\n",
    "# Print paths to verify\n",
    "for key, path in paths.items():\n",
    "    print(f\"{key} config path: {path['config']}\")\n",
    "    print(f\"{key} ckpt path: {path['ckpt']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_aug_input = True\n",
    "\n",
    "(\n",
    "    X,\n",
    "    y,\n",
    "    model,\n",
    "    datamodule,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    test_dataloader,\n",
    "    cfg_dict,\n",
    "    x_sample_train_tensor,\n",
    "    x_sample_test_tensor,\n",
    ") = get_X_y_arrays(\n",
    "    paths[\"forest\"][\"config\"],\n",
    "    paths[\"forest\"][\"ckpt\"],\n",
    "    layer,\n",
    "    downsample_method,\n",
    "    getitem_keys,\n",
    "    device,\n",
    "    n_batches_to_process,\n",
    "    mode=\"holdout\",\n",
    "    verbose=False,\n",
    "    collect_aug_input=collect_aug_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample_train_tensor = torch.Tensor(x_sample_train_tensor).to(\"cuda\")\n",
    "x_sample_test_tensor = torch.Tensor(x_sample_test_tensor).to(\"cuda\")\n",
    "print(x_sample_train_tensor.shape, x_sample_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layer_names = get_all_layer_names(model)\n",
    "\n",
    "conv_layers = [layer for layer in all_layer_names if \"conv\" in layer]\n",
    "selected_layers = pick_random_layers(all_layer_names, 10)\n",
    "selected_layers = [[layer] for layer in selected_layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g\", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k_condidence_g_hat(\n",
    "    X,\n",
    "    y,\n",
    "    M,\n",
    "    test_size=0.2,\n",
    "    n_runs=10,\n",
    "    confidence_level=0.95,\n",
    "    confidence_intervals_g=confidence_intervals_g,\n",
    "    clf=None,\n",
    "    save_plot=True,\n",
    "    fname=cfg_dict[\"data\"][\"class_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster,\n",
    "    y_train_cluster,\n",
    "    y_clusters,\n",
    "    class_name=cfg_dict[\"data\"][\"class_name\"],\n",
    "    save_plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HerbaceousVegetation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, model, datamodule, train_dataloader, val_dataloader, test_dataloader, cfg_dict = (\n",
    "    get_X_y_arrays(\n",
    "        paths[\"herb_veg\"][\"config\"],\n",
    "        paths[\"herb_veg\"][\"ckpt\"],\n",
    "        layer,\n",
    "        downsample_method,\n",
    "        getitem_keys,\n",
    "        device,\n",
    "        n_batches_to_process,\n",
    "        mode=\"holdout\",\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g: \", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k_condidence_g_hat(\n",
    "    X,\n",
    "    y,\n",
    "    M,\n",
    "    test_size=0.2,\n",
    "    n_runs=10,\n",
    "    confidence_level=0.95,\n",
    "    confidence_intervals_g=confidence_intervals_g,\n",
    "    clf=None,\n",
    "    save_plot=True,\n",
    "    fname=cfg_dict[\"data\"][\"class_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster,\n",
    "    y_train_cluster,\n",
    "    y_clusters,\n",
    "    class_name=cfg_dict.data.class_name,\n",
    "    save_plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, model, datamodule, train_dataloader, val_dataloader, test_dataloader, cfg_dict = (\n",
    "    get_X_y_arrays(\n",
    "        paths[\"highway\"][\"config\"],\n",
    "        paths[\"highway\"][\"ckpt\"],\n",
    "        layer,\n",
    "        downsample_method,\n",
    "        getitem_keys,\n",
    "        device,\n",
    "        n_batches_to_process,\n",
    "        mode=\"holdout\",\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g: \", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k_condidence_g_hat(\n",
    "    X,\n",
    "    y,\n",
    "    M,\n",
    "    test_size=0.2,\n",
    "    n_runs=10,\n",
    "    confidence_level=0.95,\n",
    "    confidence_intervals_g=confidence_intervals_g,\n",
    "    clf=None,\n",
    "    save_plot=True,\n",
    "    fname=cfg_dict[\"data\"][\"class_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster,\n",
    "    y_train_cluster,\n",
    "    y_clusters,\n",
    "    class_name=cfg_dict.data.class_name,\n",
    "    save_plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industrial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, model, datamodule, train_dataloader, val_dataloader, test_dataloader, cfg_dict = (\n",
    "    get_X_y_arrays(\n",
    "        paths[\"industrial\"][\"config\"],\n",
    "        paths[\"industrial\"][\"ckpt\"],\n",
    "        layer,\n",
    "        downsample_method,\n",
    "        getitem_keys,\n",
    "        device,\n",
    "        n_batches_to_process,\n",
    "        mode=\"holdout\",\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g: \", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k_condidence_g_hat(\n",
    "    X,\n",
    "    y,\n",
    "    M,\n",
    "    test_size=0.2,\n",
    "    n_runs=10,\n",
    "    confidence_level=0.95,\n",
    "    confidence_intervals_g=confidence_intervals_g,\n",
    "    clf=None,\n",
    "    save_plot=True,\n",
    "    fname=cfg_dict[\"data\"][\"class_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster,\n",
    "    y_train_cluster,\n",
    "    y_clusters,\n",
    "    class_name=cfg_dict.data.class_name,\n",
    "    save_plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, model, datamodule, train_dataloader, val_dataloader, test_dataloader, cfg_dict = (\n",
    "    get_X_y_arrays(\n",
    "        paths[\"pasture\"][\"config\"],\n",
    "        paths[\"pasture\"][\"ckpt\"],\n",
    "        layer,\n",
    "        downsample_method,\n",
    "        getitem_keys,\n",
    "        device,\n",
    "        n_batches_to_process,\n",
    "        mode=\"holdout\",\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g\", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k_condidence_g_hat(\n",
    "    X,\n",
    "    y,\n",
    "    M,\n",
    "    test_size=0.2,\n",
    "    n_runs=10,\n",
    "    confidence_level=0.95,\n",
    "    confidence_intervals_g=confidence_intervals_g,\n",
    "    clf=None,\n",
    "    save_plot=True,\n",
    "    fname=cfg_dict[\"data\"][\"class_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster,\n",
    "    y_train_cluster,\n",
    "    y_clusters,\n",
    "    class_name=cfg_dict[\"data\"][\"class_name\"],\n",
    "    save_plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permanent Crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, model, datamodule, train_dataloader, val_dataloader, test_dataloader, cfg_dict = (\n",
    "    get_X_y_arrays(\n",
    "        paths[\"permanentcrop\"][\"config\"],\n",
    "        paths[\"permanentcrop\"][\"ckpt\"],\n",
    "        layer,\n",
    "        downsample_method,\n",
    "        getitem_keys,\n",
    "        device,\n",
    "        n_batches_to_process,\n",
    "        mode=\"holdout\",\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g: \", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k(\n",
    "    X, y, M, confidence_intervals_g=confidence_intervals_g\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster, y_train_cluster, y_clusters, class_name=cfg.data.class_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, model, datamodule, train_dataloader, val_dataloader, test_dataloader, cfg_dict = (\n",
    "    get_X_y_arrays(\n",
    "        paths[\"residential\"][\"config\"],\n",
    "        paths[\"residential\"][\"ckpt\"],\n",
    "        layer,\n",
    "        downsample_method,\n",
    "        getitem_keys,\n",
    "        device,\n",
    "        n_batches_to_process,\n",
    "        mode=\"holdout\",\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g: \", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k(\n",
    "    X, y, M, confidence_intervals_g=confidence_intervals_g\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster, y_train_cluster, y_clusters, class_name=cfg.data.class_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# River"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, model, datamodule, train_dataloader, val_dataloader, test_dataloader, cfg_dict = (\n",
    "    get_X_y_arrays(\n",
    "        paths[\"river\"][\"config\"],\n",
    "        paths[\"river\"][\"ckpt\"],\n",
    "        layer,\n",
    "        downsample_method,\n",
    "        getitem_keys,\n",
    "        device,\n",
    "        n_batches_to_process,\n",
    "        mode=\"holdout\",\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g: \", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k(\n",
    "    X, y, M, confidence_intervals_g=confidence_intervals_g\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster, y_train_cluster, y_clusters, class_name=cfg.data.class_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeaLake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, model, datamodule, train_dataloader, val_dataloader, test_dataloader, cfg_dict = (\n",
    "    get_X_y_arrays(\n",
    "        paths[\"sealake\"][\"config\"],\n",
    "        paths[\"sealake\"][\"ckpt\"],\n",
    "        layer,\n",
    "        downsample_method,\n",
    "        getitem_keys,\n",
    "        device,\n",
    "        n_batches_to_process,\n",
    "        mode=\"holdout\",\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g: \", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k(\n",
    "    X, y, M, confidence_intervals_g=confidence_intervals_g\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster, y_train_cluster, y_clusters, class_name=cfg.data.class_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annual Crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, model, datamodule, train_dataloader, val_dataloader, test_dataloader, cfg_dict = (\n",
    "    get_X_y_arrays(\n",
    "        paths[\"annualcrop\"][\"config\"],\n",
    "        paths[\"annualcrop\"][\"ckpt\"],\n",
    "        layer,\n",
    "        downsample_method,\n",
    "        getitem_keys,\n",
    "        device,\n",
    "        n_batches_to_process,\n",
    "        mode=\"holdout\",\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.class_name + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g\", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k_condidence_g_hat(\n",
    "    X,\n",
    "    y,\n",
    "    M,\n",
    "    test_size=0.2,\n",
    "    n_runs=10,\n",
    "    confidence_level=0.95,\n",
    "    confidence_intervals_g=confidence_intervals_g,\n",
    "    clf=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster, y_train_cluster, y_clusters, class_name=cfg.data.class_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for downsample in downsample_methods:\n",
    "    print(f\"Running experiments for {downsample} downsample method.\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=collect_activations_from_layers,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    print(\"Run g experiment for downsample method:\", downsample)\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.__class__.__name__ + \"_\" + downsample\n",
    "    downsample_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_downsample_benchmark(downsample_benchmark, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in selected_layers:\n",
    "    print(f\"Running experiments for layer {lyr}\")\n",
    "    X, y, test_property_lengths = create_feature_matrix_and_labels(\n",
    "        model=model,\n",
    "        dm=datamodule,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        layer_names=lyr,\n",
    "        device=device,\n",
    "        getitem_keys=getitem_keys,\n",
    "        n_batches_to_process=n_batches_to_process,\n",
    "        downsample_method=downsample_method,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    g_benchmark = run_g_experiment(\n",
    "        X, y, split_seed, test_size, n_estimators, fixed_classifier_seed, clf=None\n",
    "    )\n",
    "    dict_key = datamodule.__class__.__name__ + \"_\" + str(lyr)\n",
    "\n",
    "    layer_benchmark[dict_key] = g_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_benchmark(layer_benchmark, all_layer_names, datamodule.class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_benchmark = run_multiple_experiments_g(\n",
    "    X, y, test_size, n_estimators, random_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for the specified columns\n",
    "columns_of_interest = [\"baseline_accuracy\", \"baseline_fpr95\", \"baseline_roc_auc\"]\n",
    "confidence_intervals_g = calculate_confidence_intervals(\n",
    "    g_benchmark, columns_of_interest\n",
    ")\n",
    "confidence_intervals_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_g_hat = run_optuna_study(\n",
    "    X,\n",
    "    y,\n",
    "    n_optuna_trials,\n",
    "    test_size,\n",
    "    min_cluster,\n",
    "    max_cluster_ratio,\n",
    "    min_fraction,\n",
    "    max_fraction,\n",
    "    n_estimators,\n",
    "    fixed_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = results_df_g_hat.to_dict(orient=\"records\")\n",
    "k = int(len(X) * 0.3)\n",
    "M = results_dict[0][\"M\"]\n",
    "print(\"k, M, len(X), k/len(X)\", k, M, len(X), k / len(X))\n",
    "\n",
    "g_hat_benchmark = run_multiple_experiments_g_hat(X, y, test_size, k, M, N, fixed_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g g_hat mean std + ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tests_results = perform_benchmark_analysis(g_benchmark, g_hat_benchmark)\n",
    "t_tests_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForestUnblanaced\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(probability=True, class_weight=\"balanced\", random_state=42),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "classifier_benchmark_df = benchmark_classifiers(\n",
    "    X, y, test_size, k, M, classifiers, fixed_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = {\n",
    "    \"KMeans\": KMeans(n_clusters=3, init=\"k-means++\", random_state=42),\n",
    "    \"DBSCAN_eps_0.1\": DBSCAN(eps=0.1, min_samples=5),\n",
    "    \"DBSCAN_eps_0.2\": DBSCAN(eps=0.2, min_samples=5),\n",
    "    \"DBSCAN_eps_0.5\": DBSCAN(eps=0.5, min_samples=5),\n",
    "}\n",
    "\n",
    "# A fixed classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "\n",
    "clustering_benchmark_df = benchmark_clustering_methods(\n",
    "    X, y, test_size, k, M, clustering_methods, classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate k wrt g-g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confidence_intervals_g: \", confidence_intervals_g)\n",
    "\n",
    "df_results = benchmark_kmeans_with_varying_k(\n",
    "    X, y, M, confidence_intervals_g=confidence_intervals_g\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Level Visualization: Understand how clustering alters the structure of the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, X_train_cluster, y_train_cluster, y_clusters = run_g_hat_experiment(\n",
    "    X, y, test_size, k, M, fixed_seed, fixed_seed, i + 1\n",
    ")\n",
    "print(\"Plotting\")\n",
    "plot_tsne_with_label_changes(\n",
    "    X_train_cluster, y_train_cluster, y_clusters, class_name=cfg.data.class_name\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
